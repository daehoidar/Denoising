# Denoising
고등학교 프로젝트
## 20111 김영석 20213 박민우

### 1.	탐구/개발 동기
학교에서 물리Ⅰ 파동에 대해 배우고 나서, 이전에는 파동에 대해 몰라서 엄두도 못냈던 음성처리 분야의 모델을 이젠 해 볼 수 있지 않을까 하여 모델 개발을 시작하였습니다. 주제 선정은 파동의 간섭 파트에 나오는 노이즈 캔슬링에서 참고하여 디노이징을 인공지능으로 구현해보면 좋겠다 생각하여 파이썬을 이용하여 개발을 시작하게 되었습니다. 저희의 희망 진로가 인공지능 개발자인 만큼, 디노이징 인공지능은 소음이 있는 공간의 통화에서 소음 제거 또는 영상 편집에서 사람의 목소리 외 배경음 제거 기술에 유용하게 쓰일 수 있기 때문에 현업에서 쓰이는 인공지능을 개발하는 느낌이 들어 더욱 열심히 임하게 되었습니다.  
### 2.	인공지능 설명
#### &nbsp;&nbsp;&nbsp;1. 데이터 처리
저희는 잡음이 있는 음성 데이터를 넣으면 잡음을 제거한 음성 데이터를 출력하는 인공지능을 만들었습니다.  <br/><br/>
&nbsp;&nbsp;데이터셋은 잡음 없이 말하는 화자의 데이터로선 The Mozilla Common Voice (MCV)를 사용하였고잡은 데이터는 The UrbanSound8K dataset을 사용하였습니다. The UrbanSound8K dataset에는street music, dog bark, children playing, car horn, air conditioner, siren, drilling, engine idling, gun shot, jackhammer같은 도시에서 들을 수 있는 소리들로 구성되어 있습니다.  <br/><br/>
&nbsp;&nbsp;우선 노이즈 데이터와 잡음 없는 음성 데이터를 각각 구해 두 음성을 합친 데이터를 생성했습니다.  

<img width="70%" src="https://user-images.githubusercontent.com/45358775/187054611-11c68326-a125-4950-b8a7-ceb05729f9eb.png"/>

&nbsp;&nbsp;그 다음, 전체 계산량과 데이터셋의 크기를 줄이기 위해 음성 데이터의 파동의 진동수를 낮추고 무음 프레임을 제거하였습니다. <br/><br/>
&nbsp;&nbsp;이번에 개발하는 모델은 오디오 디노이징을 하기 위해 주로 이미지 처리에 쓰이는 CNN을 사용하려 합니다. 그런데, 음성 데이터는 2차원인 사진 데이터와 달리 1차원 데이터이기 때문에 2차원 데이터로 바꿔줄 필요가 있습니다. 그래서 STFT(Short Time Fourier Transform, 주파수 특성이 시간에 따라 달라지는 음성을 분석하기 위한 방법)를 사용하여 1차원의 음성 데이터를 시간-주파수의 차원 데이터로 변형시킵니다. 데이터를 모델에 넣을 때 입력 형태는 (129, 8)로 하는데, 256 포인트의 생성된 벡터 STFT 데이터를 대칭으로 절반 제거함으로써 129 포인트로 줄였고, 연속된 잡음 벡터 STFT 데이터가 8개이기 때문입니다.  <br/><br/>
#### &nbsp;&nbsp;&nbsp;2. 모델 설명
이 프로젝트에서 사용된 모델의 아키텍쳐는 대칭 구조의 인코더-디코더 모델을 사용하였습니다. 각 인코더와 디코더는 ‘합성곱 연산 - ReLU - 배치 정규화 ‘로 구성된 블록으로 이루어져 있습니다. 또한 스킵 커넥션을 사용하여 모델이 학습할 때 수렴 속도를 높여주고 경사가 사라지는 문제를 줄여줍니다. 그리고 합성곱 연산을 주파수 축에만 하여 주파수 축이 순전파 중 일정하게 남아있도록 했습니다. <br/><br/>
&nbsp;&nbsp;모델의 학습은 인공 신경망에 음성과 노이즈를 합친 오디오를 넣으면 인공 신경망이 노이즈를 제거한 오디오를 예측하고 그 예측한 오디오를 원래부터 노이즈가 없던 오디오와 함께 MSE(Mean Square Error)로 평가합니다. 아래는 이 과정을 시각화한 것입니다. 

<img width="60%" src="https://user-images.githubusercontent.com/45358775/187054704-3d7b302a-fd5a-4dff-96e4-a8828a6d8d08.png"/>  

### 3. 프로젝트 결과
약 4000 개의 전체 데이터 학습을 1000번 정도 진행한 후 노이즈가 있는 음성 파일을 입력한 후 나온 결과값을 오디오 파일로 변환한 결과 아래의 그림의 Denoised Audio가 나왔습니다. 오차율을 줄이기 위해 여러 방법을 써봤음에도, 깨끗한 음성데이터, 노이즈가 첨가된 음성 데이터, 디노이즈된 음성의 그래프에서도 볼 수 있는 것과 같이 학습 결과는 딱히 좋지는 못하였습니다.

<img width="50%" src="https://user-images.githubusercontent.com/45358775/187054712-4f8f0e04-2d44-4656-8445-ccd2cdc62094.png"/>

### 4. 느낀 점
우리의 희망 진로인 인공지능과 물리 시간에 배운 것을 연관지어 프로젝트를 할 수 있어 유용한 시간을 보낼 수 있었습니다. 특히 이번에 음향 처리는 처음 해 보는데, 물리 시간에 음성의 기본이 되는 파동의 성질에 대해 배웠기 때문에 관련 지식을 습득하는데 큰 도움이 되었습니다. 하지만 푸리에 변환 같은 것들은 대학 수학도 나오고 그래서 이해하는데 큰 어려움을 겪었습니다. 또한 음성 처리는 이번이 처음이었기 때문에 모르는 지식들이 많아서 모델을 구현하는데 되게 힘들었습니다. 그래서 프로젝트의 결과도 만족스럽지 못하게 나온 것 같아 많이 아쉽습니다.

### 5. 참고자료  
https://arxiv.org/pdf/1609.07132.pdf
https://betterprogramming.pub/how-to-build-a-deep-audio-de-noiser-using-tensorflow-2-0-79c1c1aea299
